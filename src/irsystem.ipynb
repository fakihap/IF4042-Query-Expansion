{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CISI Extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parse Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_cisi_file(filepath):\n",
    "    documents = []\n",
    "    current_doc = {}\n",
    "    current_field = None\n",
    "    buffer = []\n",
    "\n",
    "    with open(filepath, 'r') as f:\n",
    "        for line in f:\n",
    "            line = line.rstrip()\n",
    "\n",
    "            if line.startswith('.I'):\n",
    "                if current_doc:\n",
    "                    if buffer and current_field:\n",
    "                        current_doc[current_field] = '\\n'.join(buffer).strip()\n",
    "                    documents.append(current_doc)\n",
    "\n",
    "                current_doc = {\"id\": int(line.split()[1]), \"references\": []}\n",
    "                current_field = None\n",
    "                buffer = []\n",
    "\n",
    "            elif line.startswith('.T'):\n",
    "                if buffer and current_field:\n",
    "                    current_doc[current_field] = '\\n'.join(buffer).strip()\n",
    "                current_field = 'title'\n",
    "                buffer = []\n",
    "\n",
    "            elif line.startswith('.A'):\n",
    "                if buffer and current_field:\n",
    "                    current_doc[current_field] = '\\n'.join(buffer).strip()\n",
    "                current_field = 'author'\n",
    "                buffer = []\n",
    "\n",
    "            elif line.startswith('.W'):\n",
    "                if buffer and current_field:\n",
    "                    current_doc[current_field] = '\\n'.join(buffer).strip()\n",
    "                current_field = 'abstract'\n",
    "                buffer = []\n",
    "\n",
    "            elif line.startswith('.X'):\n",
    "                if buffer and current_field:\n",
    "                    current_doc[current_field] = '\\n'.join(buffer).strip()\n",
    "                current_field = 'references'\n",
    "                buffer = []\n",
    "\n",
    "            else:\n",
    "                if current_field == 'references':\n",
    "                    if line.strip():  \n",
    "                        parts = line.strip().split()\n",
    "                        if len(parts) == 3:\n",
    "                            ref_id, ref_type, count = map(int, parts)\n",
    "                            current_doc['references'].append(ref_id)\n",
    "                else:\n",
    "                    buffer.append(line)\n",
    "\n",
    "        if current_doc:\n",
    "            if buffer and current_field and current_field != 'references':\n",
    "                current_doc[current_field] = '\\n'.join(buffer).strip()\n",
    "            documents.append(current_doc)\n",
    "\n",
    "    return documents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Information Retrieval System"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "class IRSystem:\n",
    "    def __init__(self, isStem, isEliminateStopWords, tfMode, isIDF, isNormalized):\n",
    "        self.isStem = isStem\n",
    "        self.isEliminateStopWords = isEliminateStopWords\n",
    "        self.tfMode = tfMode\n",
    "        self.isIDF = isIDF\n",
    "        self.isNormalized = isNormalized\n",
    "        self.stemmer = PorterStemmer()\n",
    "        nltk.download('stopwords')\n",
    "        self.stopwords = set(stopwords.words('english'))\n",
    "        with open('words.txt', 'r') as file: # UBAH FILE PATH\n",
    "            self.vocabulary = [line.strip() for line in file]\n",
    "        with open('words.txt', 'r') as file:  # UBAH FILE PATH\n",
    "            self.idfweight = np.array([line.strip() for line in file])\n",
    "\n",
    "    def stem(self, text):\n",
    "        if self.isStem:\n",
    "            words = re.findall(r'\\b[a-zA-Z]+\\b', text.lower())\n",
    "            return [self.stemmer.stem(word) for word in words]\n",
    "        else:\n",
    "            cleaned_text = re.sub(r'[^a-zA-Z\\s]', ' ', text)\n",
    "            cleaned_text = cleaned_text.lower()  \n",
    "            words = [word for word in cleaned_text.split() if word]\n",
    "            return words\n",
    "    \n",
    "    def eliminateStopWords(self, list):\n",
    "        if self.isEliminateStopWords:\n",
    "            return [word for word in list if word and word not in self.stopwords]\n",
    "        return list\n",
    "    \n",
    "    def calculateTF(self, tokens):\n",
    "        weight = np.array([0 for i in range (len(self.vocabulary) + 1)])\n",
    "        unique_token = set(tokens)\n",
    "        undefined_token = 0\n",
    "\n",
    "        # calculate TF defined token\n",
    "        for token in unique_token:\n",
    "            try:\n",
    "                idx = self.vocabulary.index(token)\n",
    "                weight[idx] = tokens.count(token)\n",
    "            except ValueError:\n",
    "                undefined_token = undefined_token + 1\n",
    "                continue\n",
    "\n",
    "        weight[self.vocabulary] = undefined_token\n",
    "                        \n",
    "        max_list = np.max(weight)\n",
    "        match self.tfMode:\n",
    "            case 'natural':\n",
    "                weight = weight\n",
    "            case 'augmented':\n",
    "                weight = 0.5 + (0.5 * weight / max_list)\n",
    "            case 'logarithmic':\n",
    "                weight = 1 + np.log2(weight, where=weight > 0, out=np.zeros_like(weight, dtype=float))\n",
    "            case 'binary':\n",
    "                weight = (weight > 0).astype(int)\n",
    "\n",
    "        return weight\n",
    "            \n",
    "    def calculateIDF(self, weight):\n",
    "        if self.isIDF:\n",
    "            return weight * self.idfweight\n",
    "        return weight\n",
    "            \n",
    "    def calculateWeight(self, token):\n",
    "        weight = self.calculateTF(token)\n",
    "        weight = self.calculateIDF(weight)\n",
    "        return weight \n",
    "    \n",
    "    def expand():\n",
    "        pass\n",
    "\n",
    "    def similarity():\n",
    "        pass\n",
    "    \n",
    "    def retrieve(self, query):\n",
    "        token = self.stem(query)\n",
    "        token = self.eliminateStopWords(token)\n",
    "        weight = self.calculateWeight(token)\n",
    "\n",
    "        # Query Expansion\n",
    "        weight = self.expand(weight)\n",
    "\n",
    "        # Calculate\n",
    "        document_rank = self.similarity(weight)\n",
    "        return document_rank\n",
    "    \n",
    "class GenerativeAdversarialNetwork:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def discriminator():\n",
    "        pass\n",
    "\n",
    "    def generator():\n",
    "        pass\n",
    "\n",
    "    def forward():\n",
    "        pass\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Precalculate Document"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### List Vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "data = parse_cisi_file(\"../dataset/cisi.all\")\n",
    "stemmer = PorterStemmer()\n",
    "author_set = set()\n",
    "title_set = set()\n",
    "abstract_set = set()\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# def stem(text):\n",
    "#     cleaned_text = re.sub(r'[^a-zA-Z\\s]', ' ', text)\n",
    "#     cleaned_text = cleaned_text.lower()  \n",
    "#     words = [word for word in cleaned_text.split() if word] # [stemmer.stem(word) for word in words if word not in stop_words]\n",
    "#     return words\n",
    "\n",
    "def stem(text):\n",
    "    words = re.findall(r'\\b[a-zA-Z]+\\b', text.lower())\n",
    "    return [stemmer.stem(word) for word in words] # [stemmer.stem(word) for word in words if word not in stop_words]\n",
    "\n",
    "for item in data:\n",
    "    author = item.get('author', '')\n",
    "    abstract = item.get('abstract', '')\n",
    "    title = item.get('title', '')\n",
    "\n",
    "    if author:\n",
    "        author_set.update(stem(author))\n",
    "    if abstract:\n",
    "        abstract_set.update(stem(abstract))\n",
    "    if title:\n",
    "        title_set.update(stem(title))\n",
    "\n",
    "with open(\"../out/stemmed/full/author.txt\", \"w\") as f:\n",
    "    f.write(\"\\n\".join(sorted(author_set)))\n",
    "\n",
    "with open(\"../out/stemmed/full/title.txt\", \"w\") as f:\n",
    "    f.write(\"\\n\".join(sorted(title_set)))\n",
    "\n",
    "with open(\"../out/stemmed/full/abstract.txt\", \"w\") as f:\n",
    "    f.write(\"\\n\".join(sorted(abstract_set)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from nltk.stem import PorterStemmer\n",
    "import re\n",
    "\n",
    "stemmer = PorterStemmer()\n",
    "data = parse_cisi_file(\"../dataset/cisi.all\")\n",
    "\n",
    "for stemming in ['stemmed', 'raw']:\n",
    "    for source in ['abstract', 'author', 'title']:\n",
    "        # Open File Vocabulary\n",
    "        with open(f'../vocabulary/{stemming}/{source}.txt', 'r') as file: \n",
    "            vocabulary = [line.strip() for line in file]\n",
    "\n",
    "        library = []\n",
    "        for document in data:\n",
    "            if stemming == 'raw':\n",
    "                words = re.sub(r'[^a-zA-Z\\s]', ' ', document[source])\n",
    "                words = words.lower()  \n",
    "                library.append([word for word in words.split() if word])\n",
    "            else: \n",
    "                words = re.findall(r'\\b[a-zA-Z]+\\b', document[source].lower())\n",
    "                library.append([stemmer.stem(word) for word in words])\n",
    "\n",
    "        # Iterate\n",
    "        idf = np.array([0.0 for i in range (len(vocabulary) + 1)])\n",
    "        for i in range (len(vocabulary)):\n",
    "            count = 0\n",
    "            for documentidx in range (len(library)):\n",
    "                if vocabulary[i] in library[documentidx]:\n",
    "                    count = count + 1\n",
    "            idf[i] = np.log2(len(data)/count)\n",
    "\n",
    "        # IDF token <UNKNOWN>\n",
    "        idf[len(vocabulary)] = 1\n",
    "\n",
    "        np.savetxt(f\"../weight/idf/{stemming}/{source}.txt\", idf, fmt='%.5f')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from nltk.stem import PorterStemmer\n",
    "import re\n",
    "\n",
    "stemmer = PorterStemmer()\n",
    "data = parse_cisi_file(\"../dataset/cisi.all\")\n",
    "\n",
    "for stemming in ['stemmed', 'raw']:\n",
    "    for source in ['abstract', 'author', 'title']:\n",
    "        # Open File Vocabulary\n",
    "        with open(f'../vocabulary/{stemming}/{source}.txt', 'r') as file: \n",
    "            vocabulary = [line.strip() for line in file]\n",
    "\n",
    "        for document in data:\n",
    "            if stemming == 'raw':\n",
    "                words = re.sub(r'[^a-zA-Z\\s]', ' ', document[source])\n",
    "                words = words.lower()  \n",
    "                library.append([word for word in words.split() if word])\n",
    "            else: \n",
    "                words = re.findall(r'\\b[a-zA-Z]+\\b', document[source].lower())\n",
    "                library.append([stemmer.stem(word) for word in words])\n",
    "\n",
    "            # Calculate\n",
    "            # Inisialisasi weight = 0 untuk semua dengan dimensi sebanyak kata di vocabulary + 1 kata tambahan yaitu <UNKNOWN> token \n",
    "            idf = np.array([0.0 for i in range (len(vocabulary) + 1)])\n",
    "            \n",
    "            #TODO: recalculate weightnya\n",
    "\n",
    "            # Weight untuk <UNKNOWN> = 0\n",
    "            idf[len(vocabulary)] = 0\n",
    "\n",
    "            #TODO: Save file (Pathnya hierarkinya jangan diubah yah)\n",
    "            np.savetxt(f\"../weight/tf/{stemming}/{document['id']}/{source}.txt\", xx, fmt='%.5f')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF.IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "data = parse_cisi_file(\"../dataset/cisi.all\")\n",
    "\n",
    "for stemming in ['stemmed', 'raw']:\n",
    "    for source in ['abstract', 'author', 'title']:\n",
    "        #TODO: import idf\n",
    "        for document in data:\n",
    "            idx = data['id']\n",
    "            #TODO: import tf dengan id = idx\n",
    "            #TODO: new weight = tf * idf\n",
    "\n",
    "            #TODO: Save file (Pathnya hierarkinya jangan diubah yah)\n",
    "            np.savetxt(f\"../weight/tf-idf/{stemming}/{document['id']}/{source}.txt\", xx, fmt='%.5f')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF IDF Length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': 1,\n",
       " 'references': [1, 1, 1, 1, 1, 556, 92, 262, 1004, 1024],\n",
       " 'title': '18 Editions of the Dewey Decimal Classifications',\n",
       " 'author': 'Comaromi, J.P.',\n",
       " 'abstract': \"The present study is a history of the DEWEY Decimal\\nClassification.  The first edition of the DDC was published\\nin 1876, the eighteenth edition in 1971, and future editions\\nwill continue to appear as needed.  In spite of the DDC's\\nlong and healthy life, however, its full story has never\\nbeen told.  There have been biographies of Dewey\\nthat briefly describe his system, but this is the first\\nattempt to provide a detailed history of the work that\\nmore than any other has spurred the growth of\\nlibrarianship in this country and abroad.\"}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "data = parse_cisi_file(\"../dataset/cisi.all\")\n",
    "\n",
    "for stemming in ['stemmed', 'raw']:\n",
    "    for source in ['abstract', 'author', 'title']:\n",
    "        length = []\n",
    "        for document in data:\n",
    "            idx = data['id']\n",
    "            #TODO: import tf.idf dengan id = idx\n",
    "            #TODO: calculate math.sqrt(sum(component ** 2 for component in vector))\n",
    "\n",
    "            length.append(magnitude)\n",
    "\n",
    "        # Jadi nanti isinya length dari document 1-terakhir\n",
    "        np.savetxt(f\"../weight/tf-idf-length/{stemming}/{source}.txt\", idf, fmt='%.5f')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
