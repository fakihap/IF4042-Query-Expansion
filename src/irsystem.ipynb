{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CISI Extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parse Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_cisi_file(filepath):\n",
    "    documents = []\n",
    "    current_doc = {}\n",
    "    current_field = None\n",
    "    buffer = []\n",
    "\n",
    "    with open(filepath, 'r') as f:\n",
    "        for line in f:\n",
    "            line = line.rstrip()\n",
    "\n",
    "            if line.startswith('.I'):\n",
    "                if current_doc:\n",
    "                    if buffer and current_field:\n",
    "                        current_doc[current_field] = '\\n'.join(buffer).strip()\n",
    "                    documents.append(current_doc)\n",
    "\n",
    "                current_doc = {\"id\": int(line.split()[1]), \"references\": []}\n",
    "                current_field = None\n",
    "                buffer = []\n",
    "\n",
    "            elif line.startswith('.T'):\n",
    "                if buffer and current_field:\n",
    "                    current_doc[current_field] = '\\n'.join(buffer).strip()\n",
    "                current_field = 'title'\n",
    "                buffer = []\n",
    "\n",
    "            elif line.startswith('.A'):\n",
    "                if buffer and current_field:\n",
    "                    current_doc[current_field] = '\\n'.join(buffer).strip()\n",
    "                current_field = 'author'\n",
    "                buffer = []\n",
    "\n",
    "            elif line.startswith('.W'):\n",
    "                if buffer and current_field:\n",
    "                    current_doc[current_field] = '\\n'.join(buffer).strip()\n",
    "                current_field = 'abstract'\n",
    "                buffer = []\n",
    "\n",
    "            elif line.startswith('.X'):\n",
    "                if buffer and current_field:\n",
    "                    current_doc[current_field] = '\\n'.join(buffer).strip()\n",
    "                current_field = 'references'\n",
    "                buffer = []\n",
    "\n",
    "            else:\n",
    "                if current_field == 'references':\n",
    "                    if line.strip():  \n",
    "                        parts = line.strip().split()\n",
    "                        if len(parts) == 3:\n",
    "                            ref_id, ref_type, count = map(int, parts)\n",
    "                            current_doc['references'].append(ref_id)\n",
    "                else:\n",
    "                    buffer.append(line)\n",
    "\n",
    "        if current_doc:\n",
    "            if buffer and current_field and current_field != 'references':\n",
    "                current_doc[current_field] = '\\n'.join(buffer).strip()\n",
    "            documents.append(current_doc)\n",
    "\n",
    "    return documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/natthankrish/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "nltk.download('stopwords')\n",
    "stopwords = set(stopwords.words('english'))\n",
    "\n",
    "with open('./vocabulary/raw/abstract.txt', 'r') as file:\n",
    "    vocab = [line.strip() for line in file if line not in stopwords]\n",
    "\n",
    "with open('./vocabulary/raw/title.txt', 'r') as file:\n",
    "    vocab += [line.strip() for line in file if line not in stopwords]\n",
    "\n",
    "with open('./vocabulary/raw/author.txt', 'r') as file:\n",
    "    vocab += [line.strip() for line in file if line not in stopwords]\n",
    "\n",
    "vocab = list(set(vocab))\n",
    "\n",
    "word_to_index = {word: idx for idx, word in enumerate(vocab)}\n",
    "index_to_word = {idx: word for word, idx in word_to_index.items()}\n",
    "\n",
    "def one_hot(idx, size):\n",
    "    vec = np.zeros(size)\n",
    "    vec[idx] = 1.0\n",
    "    return vec\n",
    "\n",
    "X = np.array([one_hot(i, len(vocab)) for i in range(len(vocab))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['model.joblib']"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPRegressor\n",
    "import joblib\n",
    "\n",
    "y = X.copy()\n",
    "\n",
    "model = MLPRegressor(hidden_layer_sizes=(5), max_iter=5000, activation='relu')\n",
    "model.fit(X, y)\n",
    "joblib.dump(model, 'model.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embedding(word):\n",
    "    idx = word_to_index[word]\n",
    "    one_hot_vec = one_hot(idx, len(vocab)).reshape(1, -1)\n",
    "    hidden = model.predict(one_hot_vec)\n",
    "    return hidden.flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "vocab_embeddings = np.array([get_embedding(word) for word in vocab])\n",
    "vocab_words = np.array(vocab)\n",
    "\n",
    "np.save('vocab_embeddings.npy', vocab_embeddings)\n",
    "\n",
    "with open('vocab_words.txt', 'w', encoding='utf-8') as f:\n",
    "    for word in vocab_words:\n",
    "        f.write(word + '\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['faced', 'spending', 'reflexive', ..., 'instigates', 'attacked',\n",
       "       'prefer'], dtype='<U29')"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('retain', 1.0000000000000002), ('equilibrium', 0.9999995232919022), ('hierarchic', 0.9999954623190725)]\n"
     ]
    }
   ],
   "source": [
    "def find_closest(input_word, top_k=1):\n",
    "    input_emb = np.array(get_embedding(input_word)).reshape(1, -1)\n",
    "    sims = cosine_similarity(input_emb, vocab_embeddings)[0]  \n",
    "    top_k_indices = np.argsort(sims)[-top_k:][::-1]      \n",
    "    closest_words = [vocab_words[i] for i in top_k_indices]\n",
    "    similarities = [sims[i] for i in top_k_indices]\n",
    "    return list(zip(closest_words, similarities))\n",
    "\n",
    "print(find_closest(\"retain\", top_k=3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Information Retrieval System"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "class IRSystem:\n",
    "    def __init__(self, isStem, isEliminateStopWords, tfMode, isIDF, isNormalized):\n",
    "        self.isStem = isStem\n",
    "        self.isEliminateStopWords = isEliminateStopWords\n",
    "        self.tfMode = tfMode\n",
    "        self.isIDF = isIDF\n",
    "        self.isNormalized = isNormalized\n",
    "        self.stemmer = PorterStemmer()\n",
    "        nltk.download('stopwords')\n",
    "        self.stopwords = set(stopwords.words('english'))\n",
    "        with open('words.txt', 'r') as file: # UBAH FILE PATH\n",
    "            self.vocabulary = [line.strip() for line in file]\n",
    "        with open('words.txt', 'r') as file:  # UBAH FILE PATH\n",
    "            self.idfweight = np.array([line.strip() for line in file])\n",
    "\n",
    "    def stem(self, text):\n",
    "        if self.isStem:\n",
    "            words = re.findall(r'\\b[a-zA-Z]+\\b', text.lower())\n",
    "            return [self.stemmer.stem(word) for word in words]\n",
    "            cleaned_text = re.sub(r'[^a-zA-Z\\s]', ' ', text)\n",
    "            cleaned_text = cleaned_text.lower()  \n",
    "            words = [word for word in cleaned_text.split() if word]\n",
    "            return words\n",
    "    \n",
    "    def eliminateStopWords(self, list):\n",
    "        if self.isEliminateStopWords:\n",
    "            return [word for word in list if word and word not in self.stopwords]\n",
    "        return list\n",
    "    \n",
    "    def calculateTF(self, tokens):\n",
    "        weight = np.array([0 for i in range (len(self.vocabulary) + 1)])\n",
    "        unique_token = set(tokens)\n",
    "        undefined_token = 0\n",
    "\n",
    "        for token in unique_token:\n",
    "            try:\n",
    "                idx = self.vocabulary.index(token)\n",
    "                weight[idx] = tokens.count(token)\n",
    "            except ValueError:\n",
    "                undefined_token = undefined_token + 1\n",
    "                continue\n",
    "\n",
    "        weight[self.vocabulary] = undefined_token\n",
    "                        \n",
    "        max_list = np.max(weight)\n",
    "        match self.tfMode:\n",
    "            case 'natural':\n",
    "                weight = weight\n",
    "            case 'augmented':\n",
    "                weight = 0.5 + (0.5 * weight / max_list)\n",
    "            case 'logarithmic':\n",
    "                weight = 1 + np.log2(weight, where=weight > 0, out=np.zeros_like(weight, dtype=float))\n",
    "            case 'binary':\n",
    "                weight = (weight > 0).astype(int)\n",
    "\n",
    "        return weight\n",
    "            \n",
    "    def calculateIDF(self, weight):\n",
    "        if self.isIDF:\n",
    "            return weight * self.idfweight\n",
    "        return weight\n",
    "            \n",
    "    def calculateWeight(self, token):\n",
    "        weight = self.calculateTF(token)\n",
    "        weight = self.calculateIDF(weight)\n",
    "        return weight \n",
    "    \n",
    "    def expand():\n",
    "        pass\n",
    "\n",
    "    def similarity(self, weight_token):\n",
    "        # token_magnitude = weight_token.magnitude()\n",
    "        # for i in range (len(self.weight_document)):\n",
    "        #     res = self.weight_document[i] * weight_token\n",
    "        #     if (self.isNormalized):\n",
    "        #         res /= token_magnitude\n",
    "        #         res /= self.weight_document.magnitude\n",
    "        return [{\"id\": 1, \"value\": 1232}, {\"id\": 1, \"value\": 1.2},]\n",
    "    \n",
    "    def retrieve(self, query):\n",
    "        token = self.stem(query)\n",
    "        token = self.eliminateStopWords(token)\n",
    "        weight = self.calculateWeight(token)\n",
    "\n",
    "        # Query Expansion\n",
    "        weight = self.expand(weight)\n",
    "\n",
    "        # Calculate\n",
    "        document_rank = self.similarity(weight)\n",
    "        return document_rank\n",
    "    \n",
    "class GenerativeAdversarialNetwork:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def discriminator():\n",
    "        pass\n",
    "\n",
    "    def generator():\n",
    "        pass\n",
    "\n",
    "    def forward():\n",
    "        pass\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Precalculate Document"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### List Vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../out/stemmed/full/author.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 36\u001b[39m\n\u001b[32m     33\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m title:\n\u001b[32m     34\u001b[39m         title_set.update(stem(title))\n\u001b[32m---> \u001b[39m\u001b[32m36\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m../out/stemmed/full/author.txt\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mw\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[32m     37\u001b[39m     f.write(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m.join(\u001b[38;5;28msorted\u001b[39m(author_set)))\n\u001b[32m     39\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m../out/stemmed/full/title.txt\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mw\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python313\\site-packages\\IPython\\core\\interactiveshell.py:326\u001b[39m, in \u001b[36m_modified_open\u001b[39m\u001b[34m(file, *args, **kwargs)\u001b[39m\n\u001b[32m    319\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[32m0\u001b[39m, \u001b[32m1\u001b[39m, \u001b[32m2\u001b[39m}:\n\u001b[32m    320\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    321\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mIPython won\u001b[39m\u001b[33m'\u001b[39m\u001b[33mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m by default \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    322\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    323\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33myou can use builtins\u001b[39m\u001b[33m'\u001b[39m\u001b[33m open.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    324\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m326\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mio_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mFileNotFoundError\u001b[39m: [Errno 2] No such file or directory: '../out/stemmed/full/author.txt'"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "data = parse_cisi_file(\"../dataset/cisi.all\")\n",
    "stemmer = PorterStemmer()\n",
    "author_set = set()\n",
    "title_set = set()\n",
    "abstract_set = set()\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# def stem(text):\n",
    "#     cleaned_text = re.sub(r'[^a-zA-Z\\s]', ' ', text)\n",
    "#     cleaned_text = cleaned_text.lower()  \n",
    "#     words = [word for word in cleaned_text.split() if word] # [stemmer.stem(word) for word in words if word not in stop_words]\n",
    "#     return words\n",
    "\n",
    "def stem(text):\n",
    "    words = re.findall(r'\\b[a-zA-Z]+\\b', text.lower())\n",
    "    return [stemmer.stem(word) for word in words] # [stemmer.stem(word) for word in words if word not in stop_words]\n",
    "\n",
    "for item in data:\n",
    "    author = item.get('author', '')\n",
    "    abstract = item.get('abstract', '')\n",
    "    title = item.get('title', '')\n",
    "\n",
    "    if author:\n",
    "        author_set.update(stem(author))\n",
    "    if abstract:\n",
    "        abstract_set.update(stem(abstract))\n",
    "    if title:\n",
    "        title_set.update(stem(title))\n",
    "\n",
    "with open(\"../out/stemmed/full/author.txt\", \"w\") as f:\n",
    "    f.write(\"\\n\".join(sorted(author_set)))\n",
    "\n",
    "with open(\"../out/stemmed/full/title.txt\", \"w\") as f:\n",
    "    f.write(\"\\n\".join(sorted(title_set)))\n",
    "\n",
    "with open(\"../out/stemmed/full/abstract.txt\", \"w\") as f:\n",
    "    f.write(\"\\n\".join(sorted(abstract_set)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from nltk.stem import PorterStemmer\n",
    "import re\n",
    "\n",
    "stemmer = PorterStemmer()\n",
    "data = parse_cisi_file(\"../dataset/cisi.all\")\n",
    "\n",
    "for stemming in ['stemmed', 'raw']:\n",
    "    for source in ['abstract', 'author', 'title']:\n",
    "        # Open File Vocabulary\n",
    "        with open(f'../vocabulary/{stemming}/{source}.txt', 'r') as file: \n",
    "            vocabulary = [line.strip() for line in file]\n",
    "\n",
    "        library = []\n",
    "        for document in data:\n",
    "            if stemming == 'raw':\n",
    "                words = re.sub(r'[^a-zA-Z\\s]', ' ', document[source])\n",
    "                words = words.lower()  \n",
    "                library.append([word for word in words.split() if word])\n",
    "            else: \n",
    "                words = re.findall(r'\\b[a-zA-Z]+\\b', document[source].lower())\n",
    "                library.append([stemmer.stem(word) for word in words])\n",
    "\n",
    "        # Iterate\n",
    "        idf = np.array([0.0 for i in range (len(vocabulary) + 1)])\n",
    "        for i in range (len(vocabulary)):\n",
    "            count = 0\n",
    "            for documentidx in range (len(library)):\n",
    "                if vocabulary[i] in library[documentidx]:\n",
    "                    count = count + 1\n",
    "            idf[i] = np.log2(len(data)/count)\n",
    "\n",
    "        # IDF token <UNKNOWN>\n",
    "        idf[len(vocabulary)] = 1\n",
    "\n",
    "        np.savetxt(f\"../weight/idf/{stemming}/{source}.txt\", idf, fmt='%.5f')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from nltk.stem import PorterStemmer\n",
    "import re\n",
    "import os\n",
    "stemmer = PorterStemmer()\n",
    "data = parse_cisi_file(\"../dataset/cisi.all\")\n",
    "\n",
    "for stemming in ['stemmed', 'raw']:\n",
    "    for source in ['abstract', 'author', 'title']:\n",
    "        # Open File Vocabulary\n",
    "        with open(f'../vocabulary/{stemming}/{source}.txt', 'r') as file: \n",
    "            vocabulary = [line.strip() for line in file]\n",
    "\n",
    "        for document in data:\n",
    "            if stemming == 'raw':\n",
    "                words = re.sub(r'[^a-zA-Z\\s]', ' ', document[source])\n",
    "                words = words.lower()  \n",
    "                tokens = [word for word in words.split() if word]\n",
    "            else: \n",
    "                words = re.findall(r'\\b[a-zA-Z]+\\b', document[source].lower())\n",
    "                tokens = [stemmer.stem(word) for word in words]\n",
    "\n",
    "            # Calculate\n",
    "            # Inisialisasi weight = 0 untuk semua dengan dimensi sebanyak kata di vocabulary + 1 kata tambahan yaitu <UNKNOWN> token \n",
    "            tf= np.array([0.0 for i in range (len(vocabulary) + 1)])\n",
    "            \n",
    "            #TODO: recalculate weightnya\n",
    "            count_word = {}\n",
    "            for token in tokens :\n",
    "                count_word[token] = count_word.get(token, 0) + 1\n",
    "                \n",
    "            for tfType in ['natural', 'augmented', 'logarithmic', 'binary'] :\n",
    "                for idx, vocab_word in enumerate(vocabulary):\n",
    "                    freq = count_word.get(vocab_word, 0)\n",
    "                    if tfType == 'natural':\n",
    "                        tf[idx] = freq\n",
    "                    elif tfType == 'augmented':\n",
    "                        max_tf = max(count_word.values()) if count_word else 1\n",
    "                        tf[idx] = 0.5 + 0.5 * (freq / max_tf) if freq > 0 else 0\n",
    "                    elif tfType == 'logarithmic':\n",
    "                        tf[idx] = (1 + np.log10(freq)) if freq > 0 else 0\n",
    "                    elif tfType == 'binary':\n",
    "                        tf[idx] = 1 if freq > 0 else 0\n",
    "                    \n",
    "\n",
    "\n",
    "                # Weight untuk <UNKNOWN> = 0\n",
    "                tf[len(vocabulary)] = 0\n",
    "\n",
    "                #TODO: Save file (Pathnya hierarkinya jangan diubah yah)\n",
    " \n",
    "                os.makedirs(f\"../weight/tf/{tfType}/{stemming}/{document['id']}\", exist_ok=True)\n",
    "                # Simpan file\n",
    "                # np.savetxt(f\"{save_path}/{source}.txt\", tf, fmt='%.5f')\n",
    "                np.savetxt(f\"../weight/tf/{tfType}/{stemming}/{document['id']}/{source}.txt\", tf, fmt='%.5f')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF.IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "\n",
    "data = parse_cisi_file(\"../dataset/cisi.all\")\n",
    "\n",
    "for stemming in ['stemmed', 'raw']:\n",
    "    for source in ['abstract', 'author', 'title']:\n",
    "        \n",
    "        idf_path = f\"../weight/idf/{stemming}/{source}.txt\"\n",
    "        idf = np.loadtxt(idf_path)\n",
    "\n",
    "        for document in data:\n",
    "            doc_id = document['id']\n",
    "\n",
    "            for tfType in ['natural', 'augmented', 'logarithmic', 'binary']:\n",
    "                # Load TF\n",
    "                tf_path = f\"../weight/tf/{tfType}/{stemming}/{doc_id}/{source}.txt\"\n",
    "                tf = np.loadtxt(tf_path)\n",
    "\n",
    "                # Hitung TF-IDF\n",
    "                tf_idf = tf * idf\n",
    "\n",
    "                # Simpan hasil\n",
    "                os.makedirs(f\"../weight/tf-idf/{tfType}/{stemming}/{doc_id}\", exist_ok=True)\n",
    "\n",
    "                np.savetxt( f\"../weight/tf-idf/{tfType}/{stemming}/{doc_id}/{source}.txt\", tf_idf, fmt=\"%.5f\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF IDF Length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "data = parse_cisi_file(\"../dataset/cisi.all\")\n",
    "\n",
    "for stemming in ['stemmed', 'raw']:\n",
    "    for source in ['abstract', 'author', 'title']:\n",
    "        length = []\n",
    "        for document in data:\n",
    "            idx = document['id']\n",
    "            for tfType in ['natural', 'augmented', 'logarithmic', 'binary']:\n",
    "                #TODO: import tf.idf dengan id = idx\n",
    "                tf_idf_path =  f\"../weight/tf-idf/{tfType}/{stemming}/{doc_id}/{source}.txt\"\n",
    "                tf_idf = np.loadtxt(tf_idf_path)\n",
    "                #TODO: calculate math.sqrt(sum(component ** 2 for component in vector))\n",
    "\n",
    "                magnitude = np.sqrt(np.sum(tf_idf ** 2))\n",
    "                length.append(magnitude)\n",
    "\n",
    "                \n",
    "                # Jadi nanti isinya length dari document 1-terakhir\n",
    "                os.makedirs(f\"../weight/tf-idf-length/{tfType}/{doc_id}/{stemming}\", exist_ok=True)\n",
    "                np.savetxt(f\"../weight/tf-idf-length/{tfType}/{doc_id}/{stemming}/{source}.txt\", idf, fmt='%.5f')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
